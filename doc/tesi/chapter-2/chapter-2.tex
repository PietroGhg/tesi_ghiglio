\section{Overview}

This chapter will provide an overview of the state of the art methods to measure, estimate and visualize the energy consumption of software. \par
In general, there is no unique solution to this problem: the proposed techniques vary by both the applicative domain, the properties of the result and the procedure with which the result is obtained. \par
For the applicative domains, we have identified three main cases:
\begin{itemize}
\item Embedded systems: embedded systems are often employed as sensors in contexts where the only source of electricity is their own battery. Therefore, energy consumption has always been a concern of both hardware and software developers.
\item Smart phones: similarly to embedded systems, smart phones have to relay on their own battery to operate. The current rate of battery improvement is around 5\% a year, but the workloads that smart phones have to withstand increases by an order of magnitude every 5 years \cite{nunez}. This means that energy consumption has to be tackled also from the software prospective.
\item Multicore CPU: energy consumption is not a big concern from the point of view of PC users. But it is a primary concern in large datacenters where heat dissipation requires good engineering solutions, and whose impact on global CO2 emission and energy consumption is non-negligible.
\end{itemize}
\par In terms of the properties of the result of the measurement/estimate, solutions differ by their granularity: some provide a single quantity (the total amount of energy consumed by a program), other have a finer grain, allowing to attribute energy measures to either source code entities or hardware components. 
\par The procedure adopted to obtain the result are widely different, \cite{rieger} provides an overview of some techniques, and groups them in simulation-based and measurement-based. Simulation based technique require a model of the target architecture, and, provided a segment of binary code, perform a cycle-accurate simulation of the events that occur during the program's run. \newline
Measurement-based techniques, instead, can be further sub-grouped in roughly three categories:
\begin{itemize}
\item Direct measurement: the measure can be taken by plugging the device to an instrument that allows to measure the current or power absorbed by the device while running the program.
\item Performance-counter based: some hardware architectures provide special register that store information about the energy consumption, and a set of APIs that allow to read their contents.
\item Modeling based: some solutions propose to model mathematically the energy behavior of the target architecture, perform some experiments in order to estimate the parameters of the model, and use the model in order to obtain an estimate of the consumed energy.
\end{itemize} 
\par The dimensions that we have indicated are not completely orthogonal. In particular, modeling based approaches usually target embedded devices, since they have simpler underlying architectures that are inherently easier to analyze. \newline Performance counter based method, instead, are bound to specific architectures that provide such counters, such as Intel's Running Average Power Limit (RAPL), for their Sandy Bridge architectures, the Intel System Management Controller (SMC) for the Xeon Phi, or NVidia's NVidia Management Library (NVML), that allows to obtain energy consumption of their GPUs.

\section{Simulation}
Simulation based methods are methods that provide an estimate of the energy consumption by running the assembly code of the program in an architectural simulator. Said simulator must also have been provided with some energy/power model of the target.
\par The first proposal for such a technique has been published by Tiwari et al. in 2000 \cite{wattch}. The simulator that they developed, Wattch, is the first simulator to operate at the architectural level: it does not require the full RTL design (the Verilog of the target architecture), but relays instead on a more high-level description of the CPU. 
\par Given such a description, that includes functional units, caches, register files, memories, TLB and other components, Wattch employs a parameterizable power model that, through a cycle-accurate simulation, outputs an estimate of the consumed energy. 
The simulation is run by interfacing with the SimpleScalar \cite{simplescalar} architectural simulator. 
\par Being more high-level than RTL-based simulations, Wattch is faster and doesn't rely on the Verilog description of the target (usually not disclosed by companies), to the detriment of the accuracy of the result, since it does not model in full detail the entire logic of the target.
\newline Despite being faster than RTL-based simulators, running a binary file with Wattch is several order of magnitudes (around 10000 slower, as reported in \cite{brando2008}) slower than running the actual program, even if the latter has been instrumented. 
\par Since it's original publication, the original work on Wattch has been expanded in several ways. The main contribution in that sense has been given by Li et al., who developed a completely new power simulator, McPat \cite{mcpat}, offering more modern and advanced features than Wattch. \newline It provides the support to compute power-area integrated metrics (energy-delay-area product), models static, dynamic, and short-circuit power dissipation (whereas Wattch only modeled dynamic power dissipation), allows to model multicore architectures, that have become increasingly widespread, and also provides an XML interface to the simulator, that allows McPat to be ported to different performance simulators. 
\par Validating the correctness of the output of these tools is not an easy task: they provide a very fine grained output (the power/energy estimates for each of the CPU's sub-components), but hardware manufacturers often to not disclose design data with such a level of detail. In \cite{mcpat-valid}, the authors, that work for the IBM corporation, have access to such data, and therefore they can provide a more insightful validation of the estimates emitted by McPat. They conclude that, while the procedure employed by McPat to obtain such results is sound, the power models that it exposes are often incomplete, too high-level, or represent an implementation of the structure that differs from the core at hand. The authors provide also some guidelines to improve power modeling accuracy, but ultimately state that academic researches would greatly benefit from the availability of validated power models for contemporary commercial chips, emitted by the hardware producers themselves.
\par To conclude this section: simulation based power models are very interesting as they can characterize an hardware architecture with great detail, but their (slow) simulation speed, and some concerns regarding their accuracy, make them not practical for software developers: they are more suited to hardware/compiler developers that want to characterize the power/energy behavior of a target architecture, not to programmers that want to characterize the energy behavior of software.

\section{Direct measuring}

Directly measuring the current drawn by a device during the program's execution is the method the provides the greatest accuracy, but it's also the one that has the greatest "overhead" for a developer that wants to assess the energy consumption of his software. \newline 
Given it's accuracy, it is often used as "ground truth" when evaluating the performances of other methods (simulations, performance counter or modeling). 
\par Experimental setups may differ, depending on the target architecture and the tools at disposal, but they usually consist in a measuring point placed between the device and the power supply. For example in \cite{roth}, they state that their experimental setup consist in a precision current-sense amplifier that amplifies the voltage drop across a shunt resistor, the output signal is then sampled by an Analog to Digital Converter, and sent to a PC. \newline
Given the measured current, $I$, and the supply voltage $V_{cc}$ the power drawn by the running target is given by $P = I \times V_{cc}$. The total energy consumed is given by $E = P \times T$, where $T$ is the running time, which can be further decomposed in $T = N \times \tau$, where $N$ is the is the number of clock cycles taken by the program, and $\tau$ is the clock period \cite{tiwari}.
\par In \cite{fahad}, instead, they employed a power meter located between the target's power socket and the A/C outlet, in order to establish the energy consumption of servers running Intel multicore CPUs.
\par As we said, the main the drawback of directly measuring the energy consumption is the fact that a whole experimental setup is required, with appropriate tools that a software developer may not even have at his disposal. Another drawback of this approach is that it provides only a raw quantity (program X during this run consumed Y Joule), but a software developer may also desire some clues about which source-code entities lead to that energy consumption.

\section{Performance counters}
Performance counters are a feature of some hardware architectures. These architectures expose some registers that store information about the energy consumption of a program (among other metrics). In the following section we will give an overview of Intel's RAPL as an example of such a feature.
\par RAPL provides a set of counters providing energy and power information. It is not an analog power meter, but rather uses a software power model that estimates power consumption by means of performance counters and hardware power models \cite{rapl-link}. The RAPL counters can be accessed by a user by reading appropriate files on the target machine. 
\par A typical usage of RAPL is to read the energy measures, perform a task, and then read again the energy measures, taking the difference between the two readings as the estimate of the energy consumed by the task. 
\par RAPL provides a fine-grained view of the energy consumption, with respect of the hardware components, offering separate estimates for each of the following domain:
\begin{itemize}
\item Package: the whole CPU.
\item Core: the central components of the CPU, such as ALU, FPU and L1 and L2 cache.
\item Uncore: components that are shared between cores, such as L3 cache and the memory controller.
\item DRAM: the main memory.
\end{itemize}
A typical use case of RAPL is given in \cite{jrapl}, in which the authors first provide a java library that allows to easily access the RAPL energy estimates, and then use said library to benchmark several data access and data organization patterns, providing some guidelines for application-level energy optimizations. They follow the typical usage pattern of measurement $\rightarrow$ task $\rightarrow$ measurement. 
\par Regarding RAPL's accuracy, there are some discording opinions: in \cite{rapl}, where RAPL is introduced to the general public, they state that the prediction provided by their power model matches actual measurements, showing high correlation between the two. This is partially disproved in \cite{fahad}, where the authors compare results obtained with RAPL to results obtained via direct measurement. They show that the average error between RAPL's estimate and the actual energy consumption ranges from 8\% to 73\%. The discrepancy between the estimate and the ground truth is also non-constant: it varies greatly depending on both the performed task and the configuration of the machine. They even show that optimizing an application using data collected from RAPL as benchmark leads to an effective increase in the total (directly measured) consumed energy.
\par To summarize this section, RAPL offers a very interesting set of features: provides a fine granularity in terms of hardware components, can be accessed from source code, allowing to profile arbitrary regions of code, and has very little overhead for the programmer (he just has to add the calls to RAPL where he is interested), but it suffers of accuracy problems. This means that, at least, it is not a good candidate to be used as ground truth to validate other estimation methods.

\section{Instruction Level Energy modeling}
Given a target's Instruction Set Architecture (ISA), an energy model is a model of the energy consumed by each instruction.
They have been introduced in 1996 by Tiwari et al. \cite{tiwari}. 
\subsection{Characterization of an ISA Energy model}
The main components of an energy model are:
\begin{itemize}
\item Instruction base cost ($B_{i}$, for each instruction $i$): the cost associated with the basic processing needed to execute an instruction. 
\item Effect of circuit state ($O_{i,j}$, for each pair of instruction $i$, $j$): the cost of the switching activity resulting from executing two consecutive instructions differing one from another.
\item Other inter-instruction effects ($E_{k}$, for each additional effect $k$): any other effect that can occur in real program, such as stalls or cache misses.
\end{itemize}
\par Given these components and a program $P$, the total energy consumed by it, $E_{p}$, is given by:
\begin{gather*}
E_{p} = \sum_{i} (B_{i} \times N_{i}) + \sum_{i,j} (O_{i,j} \times N_{i,j}) + \sum_{k} E_{k}
\end{gather*}
Where $N_{i}$ is the number of occurrences of instruction $i$, and $N_{i,j}$ is the number of times there has been a switch from instruction $i$ to instruction $j$.

\subsection{Why employing an ISA energy model}
The most common way to describe a processor's power consumption is through the average power consumption. \par
This single number may not provide enough information to characterize the energy consumed by a program running on the target processor: different programs may employ the functional units of the CPU in different ways, leading to different measurements at equal running time. \par
ISA Energy Models offer a more detailed view of the energy profile of the target architecture. They therefore allow to identify variations of consumed energy from one program to another, and may also guide decision of both humans (hardware/software design) and software (compilers or operating systems).

\subsection{Producing an ISA energy model}
Energy models can be produced through an experimental procedure. \newline
In order to obtain instruction base costs, a program consisting of a large loop of a repeated instruction is written. Then one can measure the average current drawn by the processor while executing the program, $\hat{i}$, and multiply it by the supply voltage $V_{cc}$, obtaining the base energy consumption. \newline
Instruction may also be grouped together, since instruction with similar functionality will have similar base cost.\par 
In order to obtain the circuit state effects, loop of pairs of instruction are required. The difference between the instruction's base costs and the average current measured provides the circuit state overhead. \par 
A similar approach can be employed to obtain the costs of other inter-instruction effects: writing large loops in which the examined effect occurs several times, measuring the average current and subtracting the costs that are already known (base costs and circuit state). \par 
The main disadvantage of this approach is that several different programs must be written: for an ISA with $n$ instructions, $\bigO(n)$ programs are required to produce base costs and $\bigO(n^{2})$ for circuit state effects. \newline
Estimation of other inter-instruction effects also gets more difficult as the complexity of the architecture increases. \par
On the other hand, this approach has the big advantage of not requiring a model of the circuit of the target processor, information that is often not disclosed by the manufacturing companies.
\par 
In \cite{tiwari-group}, the same authors of \cite{tiwari} employ their technique to model the instruction level energy consumption of a Digital Signal Processing (DSP) embedded system. They describe their experimental setup, consisting in a standard, off-the-shelf, dual-slope integrating digital ammeter connected between the power supply and the pins of the DSP chip. They exploit this power model in order to design a scheduling algorithm that minimizes the total energy consumed. \par 
In this work, they also highlight some practical issues regarding the methodology employed to construct the energy model: impact of operand values and tables size. For the impact of operand values, they propose to make the measurement using a wide a range of operand values, and averaging the consumption values. \par 
By table size, instead, they mean that the number of experiments that need to be carried out to model all the instruction can be overwhelming, and so they propose to group instructions by similar functionality, assigning an average cost to each group, thus reducing the number of needed experiments. The variation of the energy cost of instructions grouped in this way is around 5\%. 

\subsection{Extensions}
The original work of Tiwari et al. has been extended in several ways through the years, both in terms of the complexity of the model, and in terms of how said model has been exploited. \par
In \cite{xmos}, Eder et al. characterize the energy behavior of a multithreaded architecture. They target processor is the XMOS XS1-L. In this processors threads are executing in a round robin fashion, this makes program execution time-deterministic and allows to easily model the multithreaded behavior. \par 
According to their model, the energy consumption of a program, $E_{p}$, is given by:
\begin{gather*} 
E_{p} = P_{base}N_{idle}T_{clk} + \sum_{i=1}^{N_{t}} \sum_{i \in ISA}((M_{t}P_{i}O + P_{base})N_{i,t}T_{clk})
\end{gather*}
Where $T_{clk}$ is the clock period, $P_{base}$ is dissipated power when the processor is idle, $N_{idle}$ is the number of clock cycles in which the processor was idle, $N_{t}$ is the maximum number of running threads, $M_{t}$ is a multiplier that depends of the level of concurrency, and, for each instruction $i$, $P_{i}$ is the dissipated power and $N_{i,t}$ is the amount of times instruction the instruction has been executed in thread $t$, finally, $O$ is the inter-instruction overhead, that they assume to be constant. \par 
In order to perform their experiments, they have designed a software suite that allows to automatically generate benchmarks used to characterize the energy model, loading them on the target and monitor their execution. Tests are generated only for instructions with no effects on control flow and no non-deterministic timing. They obtain execution statistics by hardware simulation (this can be replaced by profiling).
They observed that the number of operands has significant impact on power consumption, while data width has an also an impact on power consumption, but way lower. They also choose to generalize inter-instruction overhead, observing that it exhibits little variance between different couples of instructions. \par 
For instructions that cannot be directly tested, they propose two solutions: either group instructions by number of operands, and put the untestable instr in the appropriate group or assign default cost to untestable instr. \newline
They conclude by stating that the model in which instructions are grouped by number of operands performs worse than the model in which instruction are considered individually: the first one exhibits an average error of 16\%, the latter 7\%). Both the models provide a consistent underestimation. \\[1in]


In \cite{lee}, Lee et al. propose a different methodology to construct the energy model of RISC architectures, targeting embedded systems, that combines an empirical method with statistical data analysis. \par
They state that techniques such the one proposed by \cite{tiwari} are too simplistic, since they relay only on average current, and therefore cannot consider effects such as the operand specifiers or the instruction fetch address, which may give a contribution to the total energy consumption. \par 
They firstly developed a linear model, then they estimate the unknowns of the model thanks to data from empirical observations, through linear regression. \par 
In their model, they consider a pipelined processor with $S$ stages, and $e_{s}(X,Y)$ is the energy consumed in pipeline stage $s$ when instruction $X$ is executed after instruction $Y$. This allows them to consider switching activity between different instructions. They indicate with $I_{s}(i)$ the instruction executed in pipeline stage $s$ during clock cycle $i$, and compute the energy consumed in cycle $i$ as the sum of the energy consumed by the pipeline stages: $E_{i} = \sum_{s \in S} e_{s}(I_{s}(i),I_{s}(I-1))$. \par 
In order to estimate $e_{s}(X,Y)$, they consider a set $V$ of \emph{instruction level model variables}, such as instruction fetch address, instruction bit encoding, operand specifiers (registers numbers or immediates) and data values. Each instruction $X$ is characterized by a set of natural numbers, one for each model variable, and they indicate one of such numbers as $v_{X}$. Given these variables and the corresponding quantities, they compute $e_{s}(X,Y)$ as the sum of the base cost of instruction $X$ in stage $s$, $B_{s}^{X}$, and, for each model variable $v$, the variation of energy consumption contributed by it: $f_{s}^{X}(v_{X}, v_{Y})$. So we have $e_{s}(X,Y) =  B_{s}^{X} + \sum_{v \in V}f_{s}^{X}(v_{X}, v_{Y})$. \par 
The contribution of each model variable is itself expressed as a function of the Hamming distance between $v_{X}$ and $v_{Y}$, $h(v_{X}, v_{Y})$, and the number of bit with value 1 in the binary representation of $v_{X}$, called weight $w(v_{X})$: $f_{s}(v_{X}, v_{Y}) = H_{s}^{v/X} \cdot h(v_{X}, v_{Y}) + W_{s}^{v/X} \cdot w(v_{X}$, where $H_{s}^{v/X}$ and $W_{s}^{v/X}$ are unknown coefficients. \par 
Their model, therefore, has three sets of unknown parameters($B$, $H$, and $W$) that will be estimated through linear regression. In order to do so, they proceed iteratively, by designing a set of programs where only one of the model variables changes, estimate the corresponding unknowns, and use the estimated parameters in next iteration of the measuring $\rightarrow$ estimating process. \par 
To validate their work, they generated a random set of data processing operations, with random operands, and compared the result of their estimate with a direct measure, showing an error ranging from 1\% to around 6\%. \\[1in]


In \cite{brando2008} Brandolese et al. propose one the first methodologies to both estimate the energy consumption of a program and mapping it to source code level entities. The analysis is performed at level of the parse-tree. The parse tree of a C program is decorated by associating a cost-contribution (atom) to each node. The authors have also introduced \emph{kernel instructions} as a form of target independent assembly instructions, to which energy costs can be assigned. Energy cost are estimated through least square fitting, obtained by comparing to the output of the ARMulator instruction set simulator. Following the grammar's rule of the C language, rules to combine instruction costs are defined. The parse tree is then instrumented in order to produce a trace during a run of the program, containing information about which nodes have been executed. \par 
The final cost is obtained by combining the costs the of the atoms and the data from the output of the instrumentation, providing a view that maps to each node in the parse tree (which corresponds to a source level entity such as an operation, a function call or an assignment) its contribution to the overall energy cost. \par 
This approach relays on analyzing the parse tree. This allows source code visualization but binds to the source language: in order to change source language, it would be required to perform a complete analysis of the grammar rule of the new language. Also, source level entities related to the grammar of a language may not have a trivial correspondence to assembly instructions, and therefore estimating their cost in terms of the energy associated to their assembly instructions is harder. \\[1in]


The same authors of \cite{brando2008}, in \cite{brando2011} move their analysis to the LLVM-IR. They provide a technique to understand how LLVM-IR instruction are related to the target's assembly instructions, then, by means of an instrumentation that outputs a trace of the basic blocks executed during a run of a program, they gather data regarding its dynamic behavior. Finally, given a vector that maps energy costs to each assembly instruction, they are able to characterize the total energy cost of a program by summing the costs of the executed basic blocks. The cost of each basic block is obtained by summing the energy costs of all the assembly instructions corresponding to LLVM-IR instructions contained in the basic block. \par 
Their technique to understand the LLVM-IR to assembly mapping is based on statistical analysis. The correspondence is given by an $L \times K$, matrix, $T$, where $L$ is the number of LLVM instructions in the LLVM-IR language, and $K$ is the number of assembly instructions in the target's instruction set. $T_{j,k} = n$ means that in the translation of the LLVM instruction $j$, there are $n$ assembly instructions of type $k$. \newline Given a dataset of several LLVM-IR programs $S = {S_{1},...,S_{N}}$, they compile them to obtain assembly programs for the target architecture. Let $L_{i,j}$ be the numbers of LLVM instructions of type $j$ in source program $i$, and $D_{i,k}$ the number of assembly instructions of type $k$ in $i$. It's possible to build an over-constraint system of equations of type $L_{i,j} = \sum_{k=1}^{K} D_{i,k} \cdot x_{j,k}$. The unknowns $x_{j,k}$ are the elements of the sought after matrix $T$. By posing the constraint that they must be non-negative, they formulate a \emph{non-negative least square} problem, whose solution provides the LLVM-IR to assembly mapping. This analysis has to be performed every time one wants to target a different architecture. \par 
They also require the energy cost of each assembly instruction to be known. They acknowledge the fact the this information is often not disclosed by the manufacturers, and state that it may be approximated by a linear function of the clock cycles, since the  current absorbed by each clock cycle exhibits very little variance. This a very interesting claim since data about the clock cycles taken by each instruction is often available, and it may allow a very easy way to provide an estimate of the energy cost of each assembly instruction. \\[1in]


In \cite{eder}, Eder et al. provide a LLVM-IR to assembly mapping technique that differs from \cite{brando2011}, based on debug information and disassembly of the binary. Given this mapping, they provide a methodology to statically estimate the worst case energy consumption (WCEC) of a program, both at LLVM-IR level or the assembly level, and they also employ a profiling technique similar to \cite{brando2011} in order to obtain an energy consumption estimate given a run of the program. \par 
Their LLVM-IR to assembly mapping technique is based on an LLVM pass that replaces the line number information, contained in the LLVM Debug Info classes, with an unique identifier of the instruction being considered. Then, after that the modified LLVM module has been compiled, by disassembling the binary and parsing the line table, for each entry in the line table, there will a pair \emph{<addr, id>}, such that the assembly instruction at the address \emph{addr}, can be mapped to the LLVM instruction with identifier \emph{id}.
\newline This procedure by itself does not suffice in providing a complete mapping: some assembly instructions do not have a corresponding entry in the line table, but they can be safely mapped to the same LLVM instruction of the last previous assembly instruction with an entry in the line table. \par 
Their static, worst-case energy consumption estimation is based on the Implicit Path Enumeration Technique (IPET) \cite{ipet}. It requires the program's CFG, annotated with information regarding properties of the dynamic behavior of the program, such as loop bounds or mutually exclusive conditions.
Given these annotations, that must be specified by the user, the problem can be formulated as an Integer Linear Programming problem, whose cost function is $\sum_{i=0}^{N} c_{i} \cdot x_{i}$, where, for each basic block $i$, $c_{i}$ indicates the cost of executing the basic block, and $x_{i}$ indicates the number of executions of the basic block. $c_{i}$ is obtained in similar fashion to \cite{brando2011}: by adding together all the costs of the instructions in the basic blocks, which are obtained by adding the costs of the assembly instructions mapped to the LLVM instructions. \par 
Their profiling technique, instead, consist in an instrumentation that outputs the identifier of the basic block every time the basic block is run. Similarly to the static technique, the total cost is obtained by adding together the costs of the basic block executed during the run.


\section{Source Code-level visualization}

Between the aforementioned techniques, only a few of them allows for a mapping of the energy measures to source code level entities: Brandolese et al., \cite{brando2008} provide such a result, but at cost of performing their analysis on the parse tree, and the same authors in \cite{brando2011} state that their methodology can provide source code level information, but without providing examples. \par 
RAPL-based techniques, instead, allow to estimate the energy cost of arbitrary code segments by means of the measurement $\rightarrow$ task $\rightarrow$ measurement pattern, but they are coarse-grained, available only for some hardware architectures, and require the developer to manually mark the code region that they want to inspect. \par 
All the other techniques provide just a raw measure of the total energy consumption. In \cite{pereira}, Pereira et al. propose an interesting method to map such raw results to source level entities, by what they call \emph{SPectrum Based Energy Leak Localization} (SPELL). They technique is based on spectrum based fault localization, a technique that uses statistical analysis to provide hints as to which software components may be responsible for a program's failure. The authors of \cite{pereira} extend this approach, from program failures to energy leaks (a component that consumed \emph{too much} energy). Unfortunately, it is clear to understand when a program fails (the output does not match the expected output), but it is not clear to understand when the energy consumption is too high, and the authors do not clearly state how this is performed. Nevertheless, their work represents and interesting effort, since it allows to map measures obtained with various techniques (direct measuring, simulation, or performance counters), to source level entities with arbitrary granularity (packages, classes, methods). \par
There are some commercially available tool, mentioned in \cite{rieger}, that allow for source level visualization. The development board developed by SiliconLabs \cite{siliconlab} allows to perform energy measures of ARM microcontrollers, and provide a per-method visualization. The board has integrated current and voltage sensors, and periodically sends the measured values to an host computer. Binaries are statically linked and therefore the program counter provides enough information to determine the currently executed method. \par 
Other commercially available tools are oriented towards the Android world, for instance Green Droid \cite{greendroid} provides estimates of the energy consumption by exploting PowerTutor \cite{powertutor}, a component power manager that employs the battery and current sensors commonly present in smartphones.
\section{Final Remarks}
