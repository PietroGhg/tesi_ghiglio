\section{Overview}

This chapter will provide an overview of the state of the art methods to measure, estimate and visualize the energy consumption of software. \par
In general, there is no unique solution to this problem: the proposed techniques vary by both the applicative domain, the properties of the result and the procedure with which the result is obtained. \par
For the applicative domains, we have identified three main cases:
\begin{itemize}
\item Embedded systems: embedded systems are often employed as sensors in contexts where the only source of electricity is their own battery. Therefore, energy consumption has always been a concern of both hardware and software developers.
\item Smart phones: similarly to embedded systems, smart phones have to relay on their own battery to operate. The current rate of battery improvement is around 5\% a year, but the workloads that smart phones have to withstand increases by an order of magnitude every 5 years \cite{nunez}. This means that energy consumption has to be tackled also from the software prospective.
\item Multicore CPU: energy consumption is not a big concern from the point of view of PC users. But it is a primary concern in large datacenters where heat dissipation requires good engineering solutions, and whose impact on global CO2 emission and energy consumption is non-negligible.
\end{itemize}
\par In terms of the properties of the result of the measurement/estimate, solutions differ by their granularity: some provide a single quantity (the total amount of energy consumed by a program), other have a finer grain, allowing to attribute energy measures to either source code entities or hardware components. 
\par The procedure adopted to obtain the result are widely different, \cite{rieger} provides an overview of some techniques, and groups them in simulation-based and measurement-based. Simulation based technique require a model of the target architecture, and, provided a segment of binary code, perform a cycle-accurate simulation of the events that occur during the program's run. \newline
Measurement-based techniques, instead, can be further sub-grouped in roughly three categories:
\begin{itemize}
\item Direct measurement: the measure can be taken by plugging the device to an instrument that allows to measure the current or power absorbed by the device while running the program.
\item Performance-counter based: some hardware architectures provide special register that store information about the energy consumption, and a set of APIs that allow to read their contents.
\item Modeling based: some solutions propose to model mathematically the energy behavior of the target architecture, perform some experiments in order to estimate the parameters of the model, and use the model in order to obtain an estimate of the consumed energy.
\end{itemize} 
\par The dimensions that we have indicated are not completely orthogonal. In particular, modeling based approaches usually target embedded devices, since they have simpler underlying architectures that are inherently easier to analyze. \newline Performance counter based method, instead, are bound to specific architectures that provide such counters, such as Intel's Running Average Power Limit (RAPL), for their Sandy Bridge architectures, the Intel System Management Controller (SMC) for the Xeon Phi, or NVidia's NVidia Management Library (NVML), that allows to obtain energy consumption of their GPUs.

\section{Simulation}

wattch: architectural simulator that estimates CPU power consumption.
it provides cycle-level power simulation, which allows to compute maximum and average power consumption, and overall energy consumption.
parameterizable power model + resource usage counts obtained through cycle-level simulation. more high level than verilog (less accurate, but doesn't require full circuit design and is way faster). uses simplescalar as architectural simulator, providing it power models of the most common structures present in superscalar microprocessors.
it requires power models of the sub-components of a processor, such as register files, caches, functional units, reservation stations. \\[1in]

\section{Direct measuring}

Directly measuring the current drawn by a device during the program's execution is the method the provides the greatest accuracy, but it's also the one that has the greatest "overhead" for a developer that wants to assess the energy consumption of his software. \newline 
Given it's accuracy, it is often used as "ground truth" when evaluating the performances of other methods (simulations, performance counter or modeling). 
\par Experimental setups may differ, depending on the target architecture and the tools at disposal, but they usually consist in a measuring point placed between the device and the power supply. For example in \cite{roth}, they state that their experimental setup consist in a precision current-sense amplifier that amplifies the voltage drop across a shunt resistor, the output signal is then sampled by an Analog to Digital Converter, and sent to a PC. \newline
Given the measured current, $I$, and the supply voltage $V_{cc}$ the power drawn by the running target is given by $P = I \times V_{cc}$. The total energy consumed is given by $E = P \times T$, where $T$ is the running time, which can be further decomposed in $T = N \times \tau$, where $N$ is the is the number of clock cycles taken by the program, and $\tau$ is the clock period \cite{tiwari}.
\par In \cite{fahad}, instead, they employed a power meter located between the target's power socket and the A/C outlet, in order to establish the energy consumption of servers running Intel multicore CPUs.
\par As we said, the main the drawback of directly measuring the energy consumption is the fact that a whole experimental setup is required, with appropriate tools that a software developer may not even have at his disposal. Another drawback of this approach is that it provides only a raw quantity (program X during this run consumed Y Joule), but a software developer may also desire some clues about which source-code entities lead to that energy consumption.

\section{Performance counters}
Performance counters are a feature of some hardware architectures. These architectures expose some registers that store information about the energy consumption of a program (among other metrics). In the following section we will give an overview of Intel's RAPL as an example of such a feature.
\par RAPL provides a set of counters providing energy and power information. It is not an analog power meter, but rather uses a software power model that estimates power consumption by means of performance counters and hardware power models \cite{rapl-link}. The RAPL counters can be accessed by a user by reading appropriate files on the target machine. 
\par A typical usage of RAPL is to read the energy measures, perform a task, and then read again the energy measures, taking the difference between the two readings as the estimate of the energy consumed by the task. 
\par RAPL provides a fine-grained view of the energy consumption, with respect of the hardware components, offering separate estimates for each of the following domain:
\begin{itemize}
\item Package: the whole CPU.
\item Core: the central components of the CPU, such as ALU, FPU and L1 and L2 cache.
\item Uncore: components that are shared between cores, such as L3 cache and the memory controller.
\item DRAM: the main memory.
\end{itemize}
A typical use case of RAPL is given in \cite{jrapl}, in which the authors first provide a java library that allows to easily access the RAPL energy estimates, and then use said library to benchmark several data access and data organization patterns, providing some guidelines for application-level energy optimizations. They follow the typical usage pattern of measurement $\rightarrow$ task $\rightarrow$ measurement. 
\par Regarding RAPL's accuracy, there are some discording opinions: in \cite{rapl}, where RAPL is introduced to the general public, they state that the prediction provided by their power model matches actual measurements, showing high correlation between the two. This is partially disproved in \cite{fahad}, where the authors compare results obtained with RAPL to results obtained via direct measurement. They show that the average error between RAPL's estimate and the actual energy consumption ranges from 8\% to 73\%. The discrepancy between the estimate and the ground truth is also non-constant: it varies greatly depending on both the performed task and the configuration of the machine. They even show that optimizing an application using data collected from RAPL as benchmark leads to an effective increase in the total (directly measured) consumed energy.
\par To summarize this section, RAPL offers a very interesting set of features: provides a fine granularity in terms of hardware components, can be accessed from source code, allowing to profile arbitrary regions of code, and has very little overhead for the programmer (he just has to add the calls to RAPL where he is interested), but it suffers of accuracy problems. This means that, at least, it is not a good candidate to be used as ground truth to validate other estimation methods.

\section{Instruction Level Energy modeling}
Given a target's Instruction Set Architecture (ISA), an energy model is a model of the energy consumed by each instruction.
They have been introduced in 1996 by Tiwari et al. \cite{tiwari}. 
\subsection{Characterization of an ISA Energy model}
The main components of an energy model are:
\begin{itemize}
\item Instruction base cost ($B_{i}$, for each instruction $i$): the cost associated with the basic processing needed to execute an instruction. 
\item Effect of circuit state ($O_{i,j}$, for each pair of instruction $i$, $j$): the cost of the switching activity resulting from executing two consecutive instructions differing one from another.
\item Other inter-instruction effects ($E_{k}$, for each additional effect $k$): any other effect that can occur in real program, such as stalls or cache misses.
\end{itemize}
\par Given these components and a program $P$, the total energy consumed by it, $E_{p}$, is given by:
\begin{gather*}
E_{p} = \sum_{i} (B_{i} \times N_{i}) + \sum_{i,j} (O_{i,j} \times N_{i,j}) + \sum_{k} E_{k}
\end{gather*}
Where $N_{i}$ is the number of occurrences of instruction $i$, and $N_{i,j}$ is the number of times there has been a switch from instruction $i$ to instruction $j$.

\subsection{Why employing an ISA energy model}
The most common way to describe a processor's power consumption is through the average power consumption. \par
This single number may not provide enough information to characterize the energy consumed by a program running on the target processor: different programs may employ the functional units of the CPU in different ways, leading to different measurements at equal running time. \par
ISA Energy Models offer a more detailed view of the energy profile of the target architecture. They therefore allow to identify variations of consumed energy from one program to another, and may also guide decision of both humans (hardware/software design) and software (compilers or operating systems).

\subsection{Producing an ISA energy model}
Energy models can be produced through an experimental procedure. \newline
In order to obtain instruction base costs, a program consisting of a large loop of a repeated instruction is written. Then one can measure the average current drawn by the processor while executing the program, $\hat{i}$, and multiply it by the supply voltage $V_{cc}$, obtaining the base energy consumption. \newline
Instruction may also be grouped together, since instruction with similar functionality will have similar base cost.\par 
In order to obtain the circuit state effects, loop of pairs of instruction are required. The difference between the instruction's base costs and the average current measured provides the circuit state overhead. \par 
A similar approach can be employed to obtain the costs of other inter-instruction effects: writing large loops in which the examined effect occurs several times, measuring the average current and subtracting the costs that are already known (base costs and circuit state). \par 
The main disadvantage of this approach is that several different programs must be written: for an ISA with $n$ instructions, $\bigO(n)$ programs are required to produce base costs and $\bigO(n^{2})$ for circuit state effects. \newline
Estimation of other inter-instruction effects also gets more difficult as the complexity of the architecture increases. \par
On the other hand, this approach has the big advantage of not requiring a model of the circuit of the target processor, information that is often not disclosed by the manufacturing companies.
\par 
In \cite{tiwari-group}, the same authors of \cite{tiwari} employ their technique to model the instruction level energy consumption of a Digital Signal Processing (DSP) embedded system. They describe their experimental setup, consisting in a standard, off-the-shelf, dual-slope integrating digital ammeter connected between the power supply and the pins of the DSP chip. They exploit this power model in order to design a scheduling algorithm that minimizes the total energy consumed. \par 
In this work, they also highlight some practical issues regarding the methodology employed to construct the energy model: impact of operand values and tables size. For the impact of operand values, they propose to make the measurement using a wide a range of operand values, and averaging the consumption values. \par 
By table size, instead, they mean that the number of experiments that need to be carried out to model all the instruction can be overwhelming, and so they propose to group instructions by similar functionality, assigning an average cost to each group, thus reducing the number of needed experiments. The variation of the energy cost of instructions grouped in this way is around the 5\%. 


da leggere less is more\\
xmos energy model: energy modeling of multi-threaded architecture. context switching has an energy cost. extends tiwari's model by adding thread switching cost. xmprofile software suite: allows to generate load and monitor test executions. tests are generated automatically. generate tests only for instructions with no effects on control flow and no non-deterministic timing.
observes that number of operands has significant impact on power consumption. data width has an impact on power consumption (low impact). they choose to generalize inter-instruction overhead. 
for instructions that cannot be directly tested, they propose two solutions: 1) group instructions by number of operands, and put the untestable instr in the appropriate group. 2) assign default cost to untestable instr. 
they obtain execution statistics by hardware simulation (this can be replaced by profiling).
grouped model performs worse than individual model (16percent error vs 7percent error). both provide a consistent underestimation. \\[1in]


lee: energy model of risc architectures, targeting embedded systems. combining empirical method and statistical analysis.
developed a model whose unknown are estimated tanks to data from empirical observations, through linear regression. they say that modeling in tiwari's way is too simplistic, since it relays only on average current.  
First they model the energy consumption: pipelined processor, e(X,Y) is the energy when X is executed after Y (this allows to consider switching activity). it depends on several variables (v), they consider f(vx,vy): the variation on the variable v between x and y, which depends on the hemming distance between vx and vy. 
they test several sets of programs: in the first set, they test same instructions, same operands different location and determine the impact of the instruction fetch stage. in the second set, they derive instruction base cost in each of the pipeline stages. \\[1in]


nunez-yanez: system level energy/power modeling (on chip system: memory, cpu, gpu; no camera/display). they require RTL design information (often not available), and then through linear regression they produce a power model of the whole SoC. They provide high detail, being able to characterize power consumption of different components (cpu, ram, cache), but the requirement of the RTL design is too strict. \\[1in]


\cite{brando2008}: the first effort that we have found in both estimating energy consumption and mapping it to source code level entities. The analysis is performed at level of the parse-tree. The parse tree of a C program is decorated by associating a a cost-contribution (atom) to each node. The authors have also introduced \emph{kernel instructions} as a form of target independent assembly instructions, to which energy costs can be assigned. Energy cost are estimated through least square fitting, obtained by comparing to the output of the ARMulator instruction set simulator. Following the grammar's rule of the C language, rules to combine instruction costs are defined. The parse tree is then instrumented in order to produce a trace during the program's execution. \par 
The final cost is obtained by combining the costs of the of the atoms and the data from the output of the instrumentation, providing a view that maps to each node in the parse tree (which corresponds to a source level entity such as an operation, a function call or an assignment) its contribution to the overall energy cost. \par 
This approach relays on analyzing the parse tree. this allows source code visualization but binds to the source language: in order to change source language, it would be required to perform a complete analysis of the grammar rule of the new language. Also, source level entities related to the grammar of a language may not have a trivial correspondence to assembly instructions, which makes changing source language even harder. \\[1in]


\cite{brando2011}: they propose a methodology, based on LLVM-IR analysis, to estimate a program's energy consumption, and to propagate their analysis at source code level. They provide a technique to understand how LLVM-IR instruction are related to the target's assembly instruction, then, by means of an instrumentation that outputs a trace of the basic blocks executed during a run of a program, they gather data regarding the dynamic behavior of the program. Finally, given a vector of energy costs for each assembly instruction, they are able to characterize the energy cost of a program by summing the energy costs of the executed basic blocks, while the energy cost of each basic block is obtained by summing the energy costs of all the assembly instructions corresponding to LLVM-IR instructions contained in the basic block. \par 
Their technique to understand the LLVM-IR to assembly mapping is based on statistical analysis. Given a dataset of several LLVM-IR programs, they compile them to obtain assembly programs for the target architecture. Then they establish correlation between LLVM-IR instructions and assembly instructions, formulation a \emph{non-negative least square} problem. This analysis has to be performed every time one wants to target a different architecture. \par 
They also require the energy cost of each assembly instruction to be known. They acknowledge the fact the this information is often not disclosed by the manufacturers, and state that it may be approximated by a linear function of the clock cycles, since the  current absorbed by each clock cycle exhibits very little variance. This a very interesting claim since data about the clock cycles taken by each instruction is often available, and it may allow a very easy way to provide an estimate of the energy cost of each assembly instruction. \\[1in]


\cite{eder}: They provide a LLVM-IR to assembly mapping technique that differs from \cite{brando2011}, based on debug information and disassembly of the binary. Given this mapping, they provide a methodology to statically estimate the worst case energy consumption (WCEC) of a program, both at LLVM-IR level or the assembly level, and they also employ a profiling technique similar to \cite{brando2011} in order to obtain an energy estimation given a run of the program. \par 
Their LLVM-IR to assembly mapping technique is based on an LLVM pass that replaces the line number information, contained in the LLVM Debug Info classes, with an unique id of the instruction being considered. Then, after that the modified LLVM module has been compiled, by disassembling the binary and parsing the line table, for each entry in the line table, there will a pair \emph{<addr, id>}, such that the assembly instruction at the address \emph{addr}, can be mapped to the LLVM instruction with identifier \emph{id}.
\newline This procedure by itself does not suffice in providing a complete mapping: for efficiency reasons, some assembly instruction do not have a corresponding entry in the line table, but they can be safely mapped to the same LLVM instruction of the last previous assembly instruction with an entry in the line table. \par 
Their statical, worst-case energy consumption estimation is based on the Implicit Path Enumeration Technique (IPET) \cite{ipet}. It requires the program's CFG, annotated with information regarding properties of the dynamic behavior of the program, such as loop bounds or mutually exclusive conditions.
Given these annotations, that must be specified by the user, the problem can be formulated as an Integer Linear Programming problem, whose cost function is $\sum_{i=0}^{N} c_{i} \times x_{i}$, where, for each basic block $i$, $c_{i}$ indicates the cost of executing the basic block, and $x_{i}$ indicates the number of executions of the basic block. 
Their profiling technique, instead, consist in an instrumentation that outputs the identifier of the basic block every time the basic block is run. \\[1in]


rieger-survey: alcuni tool commericali, altra letteratura. molto android/java (ex. powertutor)
roth: piattaforma hardware per creare energy models + xml dell'energy model per intergrarlo nel loro compilatore, il quale fornisce una stima worst case. \\
pereira: source level view, statistical method to provide ranking, no energy estimation, just visualization. \\

\section{Source Code-level visualization}
